{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gemma-2-27b\"\n",
    "dataset = \"newsroom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4608])\n",
      "torch.Size([4, 4608])\n"
     ]
    }
   ],
   "source": [
    "probe_path = f\"/workspace/PPairS_results/{dataset}/{model_name}/probe\"\n",
    "s_probe = t.load(f\"{probe_path}_s.pt\", weights_only=True)\n",
    "u_probe = t.load(f\"{probe_path}_u.pt\", weights_only=True)\n",
    "\n",
    "print(s_probe.shape)\n",
    "print(u_probe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name: str, lora_path: str = None) -> tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    # load base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=t.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        use_cache=True\n",
    "    )\n",
    "    model.eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-27 19:03:08,243] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a15af6f94e4f8198e6c7c83cb9fa3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(f\"/workspace/models/{model_name}-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256000, 4608])\n",
      "torch.Size([256000, 4608])\n"
     ]
    }
   ],
   "source": [
    "We = model.model.embed_tokens.weight\n",
    "norm = model.model.norm\n",
    "Wu = model.lm_head.weight\n",
    "\n",
    "print(We.shape)\n",
    "print(Wu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4608])\n"
     ]
    }
   ],
   "source": [
    "print(u_probe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe direction shape: torch.Size([4608])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 most similar tokens in embedding matrix We:\n",
      "1. Token: ' RAL' (ID: 111640) - Similarity: 0.0231\n",
      "2. Token: ' viper' (ID: 119060) - Similarity: 0.0228\n",
      "3. Token: ' rattles' (ID: 151417) - Similarity: 0.0226\n",
      "4. Token: 'GetKeyDown' (ID: 183054) - Similarity: 0.0225\n",
      "5. Token: ' screenwriter' (ID: 191114) - Similarity: 0.0217\n",
      "6. Token: ' rocker' (ID: 96531) - Similarity: 0.0216\n",
      "7. Token: 'Dax' (ID: 218725) - Similarity: 0.0216\n",
      "8. Token: ' 微信' (ID: 93689) - Similarity: 0.0212\n",
      "9. Token: ' LDAP' (ID: 139984) - Similarity: 0.0212\n",
      "10. Token: 'thage' (ID: 111472) - Similarity: 0.0203\n",
      "\n",
      "Top 10 most similar tokens in unembedding matrix Wu:\n",
      "1. Token: ' RAL' (ID: 111640) - Similarity: 0.0231\n",
      "2. Token: ' viper' (ID: 119060) - Similarity: 0.0228\n",
      "3. Token: ' rattles' (ID: 151417) - Similarity: 0.0226\n",
      "4. Token: 'GetKeyDown' (ID: 183054) - Similarity: 0.0225\n",
      "5. Token: ' screenwriter' (ID: 191114) - Similarity: 0.0217\n",
      "6. Token: ' rocker' (ID: 96531) - Similarity: 0.0216\n",
      "7. Token: 'Dax' (ID: 218725) - Similarity: 0.0216\n",
      "8. Token: ' 微信' (ID: 93689) - Similarity: 0.0212\n",
      "9. Token: ' LDAP' (ID: 139984) - Similarity: 0.0212\n",
      "10. Token: 'thage' (ID: 111472) - Similarity: 0.0203\n",
      "\n",
      "Top 10 tokens via logit-lens approach:\n",
      "1. Token: 'pompa' (ID: 149784) - Logit: 42.0000\n",
      "2. Token: 'erkek' (ID: 140919) - Logit: 38.0000\n",
      "3. Token: 'reiz' (ID: 170426) - Logit: 36.5000\n",
      "4. Token: 'reportWebVitals' (ID: 209012) - Logit: 36.2500\n",
      "5. Token: 'tiara' (ID: 132134) - Logit: 36.0000\n",
      "6. Token: ' kré' (ID: 166037) - Logit: 35.7500\n",
      "7. Token: ' sokak' (ID: 234110) - Logit: 35.0000\n",
      "8. Token: 'kenzo' (ID: 203851) - Logit: 34.7500\n",
      "9. Token: '' (ID: 255796) - Logit: 34.5000\n",
      "10. Token: 'seiko' (ID: 214998) - Logit: 34.0000\n"
     ]
    }
   ],
   "source": [
    "# Average u_probe over dimension 1 to get a single direction vector\n",
    "probe_direction = u_probe.mean(dim=0)  # shape: (d_model,)\n",
    "print(f\"Probe direction shape: {probe_direction.shape}\")\n",
    "\n",
    "# Calculate cosine similarities with embedding matrix We\n",
    "we_similarities = t.nn.functional.cosine_similarity(probe_direction.unsqueeze(0), We)\n",
    "we_top_indices = we_similarities.argsort(descending=True)[:10]\n",
    "print(\"\\nTop 10 most similar tokens in embedding matrix We:\")\n",
    "for i, idx in enumerate(we_top_indices):\n",
    "    token = tokenizer.decode(idx.item())\n",
    "    similarity = we_similarities[idx].item()\n",
    "    print(f\"{i+1}. Token: '{token}' (ID: {idx.item()}) - Similarity: {similarity:.4f}\")\n",
    "\n",
    "# Calculate cosine similarities with unembedding matrix Wu\n",
    "wu_similarities = t.nn.functional.cosine_similarity(probe_direction.unsqueeze(0), Wu)\n",
    "wu_top_indices = wu_similarities.argsort(descending=True)[:10]\n",
    "print(\"\\nTop 10 most similar tokens in unembedding matrix Wu:\")\n",
    "for i, idx in enumerate(wu_top_indices):\n",
    "    token = tokenizer.decode(idx.item())\n",
    "    similarity = wu_similarities[idx].item()\n",
    "    print(f\"{i+1}. Token: '{token}' (ID: {idx.item()}) - Similarity: {similarity:.4f}\")\n",
    "\n",
    "# Logit-lens approach: apply norm then Wu\n",
    "normalized_probe = norm(probe_direction).to(t.bfloat16)\n",
    "logits = Wu @ normalized_probe\n",
    "top_logit_indices = logits.argsort(descending=True)[:10]\n",
    "print(\"\\nTop 10 tokens via logit-lens approach:\")\n",
    "for i, idx in enumerate(top_logit_indices):\n",
    "    token = tokenizer.decode(idx.item())\n",
    "    logit_value = logits[idx].item()\n",
    "    print(f\"{i+1}. Token: '{token}' (ID: {idx.item()}) - Logit: {logit_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe direction shape: torch.Size([4608])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 most similar tokens in embedding matrix We:\n",
      "1. Token: 'Summary' (ID: 9292) - Similarity: -0.0682\n",
      "2. Token: 'Sorry' (ID: 12156) - Similarity: -0.0651\n",
      "3. Token: ' Summary' (ID: 13705) - Similarity: -0.0615\n",
      "4. Token: ' Sorry' (ID: 26199) - Similarity: -0.0612\n",
      "5. Token: ' sorry' (ID: 9897) - Similarity: -0.0590\n",
      "6. Token: ' SUMMARY' (ID: 40702) - Similarity: -0.0552\n",
      "7. Token: 'sorry' (ID: 43718) - Similarity: -0.0533\n",
      "8. Token: 'Sum' (ID: 5751) - Similarity: -0.0496\n",
      "9. Token: ' summary' (ID: 13367) - Similarity: -0.0492\n",
      "10. Token: ' but' (ID: 901) - Similarity: -0.0492\n",
      "\n",
      "Top 10 most similar tokens in unembedding matrix Wu:\n",
      "1. Token: 'Summary' (ID: 9292) - Similarity: -0.0682\n",
      "2. Token: 'Sorry' (ID: 12156) - Similarity: -0.0651\n",
      "3. Token: ' Summary' (ID: 13705) - Similarity: -0.0615\n",
      "4. Token: ' Sorry' (ID: 26199) - Similarity: -0.0612\n",
      "5. Token: ' sorry' (ID: 9897) - Similarity: -0.0590\n",
      "6. Token: ' SUMMARY' (ID: 40702) - Similarity: -0.0552\n",
      "7. Token: 'sorry' (ID: 43718) - Similarity: -0.0533\n",
      "8. Token: 'Sum' (ID: 5751) - Similarity: -0.0496\n",
      "9. Token: ' summary' (ID: 13367) - Similarity: -0.0492\n",
      "10. Token: ' but' (ID: 901) - Similarity: -0.0492\n",
      "\n",
      "Top 10 tokens via logit-lens approach:\n",
      "1. Token: 'B' (ID: 235305) - Logit: -116.5000\n",
      "2. Token: 'b' (ID: 235268) - Logit: -114.0000\n",
      "3. Token: 'S' (ID: 235277) - Logit: -113.0000\n",
      "4. Token: 'c' (ID: 235260) - Logit: -112.0000\n",
      "5. Token: ' errone' (ID: 55501) - Logit: -111.5000\n",
      "6. Token: 'T' (ID: 235279) - Logit: -109.5000\n",
      "7. Token: 'F' (ID: 235311) - Logit: -109.0000\n",
      "8. Token: 'M' (ID: 235296) - Logit: -108.0000\n",
      "9. Token: 'P' (ID: 235295) - Logit: -107.5000\n",
      "10. Token: 'No' (ID: 1294) - Logit: -107.0000\n"
     ]
    }
   ],
   "source": [
    "# Average u_probe over dimension 1 to get a single direction vector\n",
    "probe_direction = u_probe.mean(dim=0)  # shape: (d_model,)\n",
    "print(f\"Probe direction shape: {probe_direction.shape}\")\n",
    "\n",
    "# Calculate cosine similarities with embedding matrix We\n",
    "we_similarities = t.nn.functional.cosine_similarity(probe_direction.unsqueeze(0), We)\n",
    "we_top_indices = we_similarities.argsort(descending=False)[:10]\n",
    "print(\"\\nTop 10 most similar tokens in embedding matrix We:\")\n",
    "for i, idx in enumerate(we_top_indices):\n",
    "    token = tokenizer.decode(idx.item())\n",
    "    similarity = we_similarities[idx].item()\n",
    "    print(f\"{i+1}. Token: '{token}' (ID: {idx.item()}) - Similarity: {similarity:.4f}\")\n",
    "\n",
    "# Calculate cosine similarities with unembedding matrix Wu\n",
    "wu_similarities = t.nn.functional.cosine_similarity(probe_direction.unsqueeze(0), Wu)\n",
    "wu_top_indices = wu_similarities.argsort(descending=False)[:10]\n",
    "print(\"\\nTop 10 most similar tokens in unembedding matrix Wu:\")\n",
    "for i, idx in enumerate(wu_top_indices):\n",
    "    token = tokenizer.decode(idx.item())\n",
    "    similarity = wu_similarities[idx].item()\n",
    "    print(f\"{i+1}. Token: '{token}' (ID: {idx.item()}) - Similarity: {similarity:.4f}\")\n",
    "\n",
    "# Logit-lens approach: apply norm then Wu\n",
    "normalized_probe = norm(probe_direction).to(t.bfloat16)\n",
    "logits = Wu @ normalized_probe\n",
    "top_logit_indices = logits.argsort(descending=False)[:10]\n",
    "print(\"\\nTop 10 tokens via logit-lens approach:\")\n",
    "for i, idx in enumerate(top_logit_indices):\n",
    "    token = tokenizer.decode(idx.item())\n",
    "    logit_value = logits[idx].item()\n",
    "    print(f\"{i+1}. Token: '{token}' (ID: {idx.item()}) - Logit: {logit_value:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
